{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_djuWRzMXkq"
   },
   "source": [
    "MODULE 5 | LESSON 1\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9dKog2Affov"
   },
   "source": [
    "##**Non-Negative Matrix Factorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Thc6NooiZzFt"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "| **Reading Time**  |  50 minutes  |\n",
    "| **Prior Knowledge**  |  Basic understanding of linear algebra and statistics: Matrices and vectors; Eigenvalues and eigenvectors; Basic statistics; <br>Familiarity with Python programming: Basic Python syntax, data manipulation and numerical operations; <br>Fundamental financial concepts: Asset returns and correlation; Portfolio diversification; Sentiment analysis  |\n",
    "| **Keywords**  | Non-negative Matrix Factorization (NMF), Dimensionality Reduction, Feature Extraction, Parts-based Representation, <br>Sparsity, Interpretability, Sparse NMF, Constrained NMF, Semi-NMF, Convex NMF, Online NMF, Sentiment Analysis, <br>Principal Component Analysis (PCA), Singular Value Decomposition (SVD), Independent Component Analysis (ICA), <br>Factor Loadings, Factor Scores |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v3SY8AiZnck"
   },
   "source": [
    "*In this lesson, we explore non-negative matrix factorization (NMF), a technique to decompose a non-negative matrix into two smaller non-negative matrices. It's often used for dimensionality reduction, feature extraction, and topic modeling. This lesson offers a comprehensive overview of NMF, its variations, and its applications, with a focus on financial engineering, particularly portfolio diversification.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "150Zf_7t3ej8"
   },
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ0IuR-Cvavo"
   },
   "source": [
    "##**1. What is Non-Negative Matrix Factorization?**\n",
    "\n",
    "Non-negative matrix factorization (NMF) is a dimensionality reduction technique that decomposes a non-negative matrix into two smaller non-negative matrices. It's often used in data analysis, machine learning, and recommender systems. Please review the required readings and come back for the remainder of this lesson.\n",
    "\n",
    "The mathematical definition of non-negative matrix factorization (NMF) is as follows:\n",
    "\n",
    "Given a non-negative matrix $V$ with dimensions $m \\times n$, NMF seeks to find two non-negative matrices:\n",
    " - $W$ with dimensions $m \\times k$ ($m$ rows, $k$ columns, where $k$ is the reduced dimensionality);\n",
    " - $H$ with dimensions $k \\times n$;\n",
    "\n",
    "Such that:\n",
    "$$V \\approx W * H$$\n",
    "   \n",
    "Where:\n",
    " - $*$ denotes matrix multiplication\n",
    " - The approximation $\\approx$ is typically measured using a cost function, such as the Frobenius norm: $\\lVert V - W * H \\rVert ^2$\n",
    "\n",
    "Constraints: All elements of $W$ and $H$ must be non-negative real numbers, i.e., $W$ ≥ 0, $H$ ≥ 0\n",
    "\n",
    "In simpler terms, we want to find two matrices ($W$ and $H$) that, when multiplied together, closely resemble our original data matrix ($V$). The elements of $W$ and $H$ must be non-negative (greater than or equal to zero).\n",
    "\n",
    "Factorization rank in NMF, often denoted as $k$, represents the number of factors or components we are trying to extract from data. It essentially determines the dimensionality of the reduced representation.\n",
    "\n",
    "**Iterative Update Rules:** There are several ways in which $W$ and $H$ may be found. NMF algorithms typically employ iterative update rules to refine the matrices $W$ and $H$ until convergence. These rules aim to minimize the difference between the original matrix $V$ and the product $W * H$. One common update rule is the multiplicative update rule, which is derived from minimizing the Frobenius norm:\n",
    "\n",
    "$$W_{ia} \\leftarrow W_{ia} * \\frac{(V * H^T)_{ia}}{(W * H * H^T)_{ia}}$$\n",
    "\n",
    "$$H_{aj} \\leftarrow H_{aj} * \\frac{(W^T * V)_{aj}}{(W^T * W * H)_{aj}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $W_{ia}$ represents the element in the $i$-th row and $a$-th column of matrix $W$.\n",
    " - $H_{aj}$ represents the element in the $a$-th row and $j$-th column of matrix $H$.\n",
    " - $V$, $W$, and $H$ are the matrices as defined in the NMF problem.\n",
    " - $H^T$ and $W^T$ denote the transpose of matrices $H$ and $W$, respectively.\n",
    " - $*$ and division represent element-wise multiplication and division, respectively.\n",
    "\n",
    "The iterative process continues until a convergence criterion is met. This can be based on:\n",
    "\n",
    " - Difference in cost function: The algorithm stops when the change in the cost function (e.g., Frobenius norm) between consecutive iterations falls below a predefined threshold.\n",
    " - Maximum number of iterations: The algorithm terminates after a fixed number of iterations, even if the cost function hasn't fully converged.\n",
    "\n",
    "Mathematically, convergence can be expressed as:\n",
    "\n",
    "$$\\lVert V - W^{(t+1)} * H^{(t+1)} \\rVert ^2 - \\lVert V - W^{(t)} * H^{(t)} \\rVert ^2 < \\varepsilon$$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $W^{(t)}$ and $H^{(t)}$ represent the matrices $W$ and $H$ at iteration $t$.\n",
    " - $\\varepsilon$ is a small positive value representing the convergence threshold.\n",
    "\n",
    "In simpler terms, the algorithm stops when the difference in the approximation error between consecutive iterations becomes sufficiently small.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX1vo2qlQBSP"
   },
   "source": [
    "## **2. Applications of Non-negative Matrix Factorization (NMF) in Financial Engineering**\n",
    "\n",
    "NMF has gained popularity in financial engineering due to its ability to extract meaningful and interpretable features from financial data. Here are some notable applications:\n",
    "\n",
    "**Portfolio diversification:** NMF can be used to identify groups of assets that exhibit similar behavior, allowing for the construction of diversified portfolios with reduced risk. By decomposing the correlation matrix of asset returns, NMF can reveal underlying factors that drive asset co-movements and help investors allocate their investments across different asset classes.\n",
    "\n",
    "**Risk management:** NMF can be used to identify and quantify risk factors in financial markets. By decomposing historical market data, NMF can uncover hidden risk factors that may not be apparent using traditional methods. This information can be used to develop risk management strategies and improve portfolio hedging.\n",
    "\n",
    "**Asset pricing:** NMF can be applied to asset pricing models to identify factors that explain asset returns. By decomposing the cross-section of asset returns, NMF can reveal underlying factors that drive asset prices and help investors understand the determinants of asset performance.\n",
    "\n",
    "**Sentiment analysis:** NMF can be used to analyze textual data, such as news articles or social media posts, to extract sentiment and gauge market sentiment toward specific assets or the overall market. This information can be used to inform investment decisions and predict market movements.\n",
    "\n",
    "**Fraud detection:** NMF can be used to detect anomalies and patterns in financial transactions that may indicate fraudulent activity. By decomposing transaction data, NMF can identify unusual patterns and highlight potential areas of concern for further investigation.\n",
    "\n",
    "**High-frequency trading:** NMF can be used to analyze high-frequency financial data and identify patterns that can be exploited for trading strategies. By decomposing price and volume data, NMF can reveal hidden relationships and predict short-term price movements.\n",
    "\n",
    "**Algorithmic trading:** NMF can be integrated into algorithmic trading systems to automate trading decisions based on extracted features and patterns in financial data. This can help improve trading efficiency and profitability.\n",
    "\n",
    "These are just a few examples of how NMF is being applied in financial engineering. Its ability to extract meaningful and interpretable features from complex financial data makes it a valuable tool for various tasks, including portfolio management, risk management, asset pricing, and trading. As financial markets become increasingly complex and data-driven, the applications of NMF in financial engineering are likely to continue to expand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC7XPQRgpLaW"
   },
   "source": [
    "## **3. Toy Example**\n",
    "\n",
    "Let's illustrate how the NMF procedure functions with this small toy example. Let's consider the following 4x5 data matrix $V$:\n",
    "\n",
    "$$V = \\begin{pmatrix}\n",
    "    1  &  2 &  3 &  4 &  5 \\\\\n",
    "    6  &  7 &  8 &  9 & 10 \\\\\n",
    "    11 & 12 & 13 & 14 & 15 \\\\\n",
    "    16 & 17 & 18 & 19 & 20 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "We want to decompose this matrix into two smaller matrices, $W$ (4x2) and $H$ (2x5), such that $V ≈ W * H$. We initialize $W^{(0)}$ (feature matrix at $t=0$) and $H^{(0)}$ (coefficient matrix at $t=0$) with random non-negative values:\n",
    "\n",
    "$$W^{(0)} = \\begin{pmatrix}\n",
    "    0.2 & 0.5 \\\\\n",
    "    0.8 & 0.3 \\\\\n",
    "    0.6 & 0.9 \\\\\n",
    "    0.4 & 0.7 \\\\\n",
    "\\end{pmatrix}, \\quad\n",
    "H^{(0)} = \\begin{pmatrix}\n",
    "    0.7 & 0.1 & 0.4 & 0.6 & 0.2 \\\\\n",
    "    0.3 & 0.6 & 0.2 & 0.4 & 0.8 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Now, we iteratively update $W^{(t)}$ and $H^{(t)}$ using the multiplicative update rules to minimize the difference between $V$ and $W^{(t)} * H^{(t)}$. After a few iterations, we might obtain the following updated matrices:\n",
    "\n",
    "$$W = \\begin{pmatrix}\n",
    "    0.9618 & 0.  \\\\\n",
    "    1.9262 & 1.0167 \\\\\n",
    "    2.8902 & 2.0347 \\\\\n",
    "    3.8542 & 3.0527 \\\\\n",
    "\\end{pmatrix}, \\quad\n",
    "H = \\begin{pmatrix}\n",
    "    1.0413 & 2.0823 & 3.1232 & 4.1642 & 5.19 \\\\\n",
    "    3.9269 & 2.9399 & 1.9529 & 0.966  & 0.     \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The final values of Feature matrix $W$ and Coefficient matrix $H$ represent the decomposed factors of the original matrix $V$. Please note that the specific values in the matrices may differ slightly depending on the actual calculations. By multiplying $W$ and $H$, we get an approximation of $V$:\n",
    "\n",
    "$$W * H = \\begin{pmatrix}\n",
    "    1.0015  &  2.0028 &  3.004  &  4.0052 &  4.9919 \\\\\n",
    "    5.9983  &  6.9999 &  8.0017 &  9.0034 &  9.9972 \\\\\n",
    "    10.9996 & 12.     & 13.0004 & 14.0008 & 15.0002 \\\\\n",
    "    16.0009 & 17.     & 17.9992 & 18.9983 & 20.0032 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "This resulting matrix is an approximation of the original matrix $V$. The difference between $V$ and $W * H$ represents the reconstruction error.\n",
    "\n",
    "$$V - W * H = \\begin{pmatrix}\n",
    "    -0.0015 & -0.0028 & -0.004  & -0.0052 &  0.0081 \\\\\n",
    "    0.0017  &  0.0001 & -0.0017 & -0.0034 &  0.0028 \\\\\n",
    "    0.0004  &  0.     & -0.0004 & -0.0008 & -0.0002 \\\\\n",
    "    -0.0009 & -0.     &  0.0008 &  0.0017 & -0.0032 \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The goal of NMF is to find $W$ and $H$ that minimize this reconstruction error while keeping all elements of $W$ and $H$ non-negative.\n",
    "\n",
    "This very simple numerical example demonstrates how NMF can decompose a larger data matrix into smaller matrices, capturing its underlying structure and relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbh5VLbiZZye"
   },
   "source": [
    "## **4. Basics of Non-Negative Matrix Factorization**\n",
    "\n",
    "\n",
    "In a nutshell, NMF is about finding a simpler representation of data by breaking it down into two parts that capture the essential features and their importance. It has applications in various fields, including image processing, text analysis, and bioinformatics. Implementation involves the following steps:\n",
    "\n",
    " - **Initialization:** Starting with a non-negative matrix, often denoted as $V$, that represents data. This is our matrix that contains information about users, products, or any other kind of data.\n",
    " - **Decomposition:** The core of NMF is to find two non-negative matrices, typically called $W$ and $H$, such that the product of $W$ and $H$ closely approximates the original matrix $V$ i.e. $V \\approx WH$. In simpler terms, we're trying to break down the original data into two parts that, when combined, resemble the original.\n",
    " - **Iteration:** NMF algorithms employ iterative processes to refine $W$ and $H$, minimizing the difference between the product $W H$ and the original matrix $V$. These iterations involve updating $W$ and $H$ repeatedly until a satisfactory level of accuracy is reached. This process is often based on gradient descent or multiplicative updates.\n",
    " - **Convergence:** The iteration process continues until a convergence criterion is met. This usually means that the difference between $W H$ and $V$ is small enough or that the algorithm has run for a predetermined number of steps.\n",
    " - **Interpretation:** After convergence, the resulting matrices $W$ and $H$ provide a lower-dimensional representation of the original data. $W$ can be seen as containing the basis vectors or features, while $H$ represents the coefficients or weights of these features for each data point. Each column of $W$ represents a feature, and the corresponding row of $H$ shows how strongly this feature is expressed in each data point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEH5ZUroGHNX"
   },
   "source": [
    "## **5. Properties of Non-negative Matrix Factorization (NMF)**\n",
    "\n",
    "Some of the key properties of non-negative matrix factorization (NMF) are:\n",
    "\n",
    "**Non-negativity:** The most fundamental property of NMF is that it produces non-negative matrices $W$ and $H$. This means that all elements of these matrices are greater than or equal to zero. This property is crucial for interpretability, as it allows us to understand the resulting factors as additive combinations of original features.\n",
    "\n",
    "**Dimensionality Reduction:** NMF reduces the dimensionality of the data by decomposing it into two lower-rank matrices. This can be useful for simplifying the data, removing noise, and identifying latent features.\n",
    "\n",
    "**Parts-based Representation:** NMF often leads to a parts-based representation of the data. This means that the resulting factors (columns of $W$) can be interpreted as representing individual parts or components of the original data. For example, in image analysis, NMF might identify factors corresponding to edges, textures, or objects.\n",
    "\n",
    "**Sparsity:** NMF often produces sparse matrices, meaning that many elements of $W$ and $H$ are zero. This can be beneficial for interpretability, as it highlights the most important features and reduces the complexity of the model.\n",
    "\n",
    "**Interpretability:** Due to its non-negativity and parts-based representation, NMF is often considered more interpretable than other dimensionality reduction techniques like Principal Component Analysis (PCA). The resulting factors can be more easily related to the original features, making it easier to understand the underlying structure of the data.\n",
    "\n",
    "**Flexibility:** NMF can be applied to a wide variety of data types, including text, images, audio, and biological data. It can also be adapted to different applications by using different cost functions and constraints.\n",
    "\n",
    "**Computational Efficiency:** NMF algorithms are generally computationally efficient, especially for sparse data. This makes them suitable for large-scale datasets.\n",
    "\n",
    "**Non-uniqueness:** The NMF decomposition is not unique, meaning that there can be multiple solutions for $W$ and $H$ that give a good approximation of $V$. This can be addressed by using regularization techniques or by imposing additional constraints.\n",
    "\n",
    "These are some of the key properties of NMF. They contribute to its popularity and effectiveness in various data analysis and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p54nwce2uP7A"
   },
   "source": [
    "## **6. Challenges and limitations of Non-negative Matrix Factorization (NMF)**\n",
    "\n",
    "Non-negative Matrix Factorization (NMF) has some challenges and limitations:\n",
    "\n",
    "**Non-Uniqueness of Solutions:** The NMF decomposition is inherently non-unique, meaning that there can be multiple pairs of factor matrices ($W$ and $H$) that, when multiplied together, approximate the original data matrix ($V$) equally well. This arises from the fact that there are often many ways to represent the same data using different combinations of non-negative factors.\n",
    "> - Implications: This non-uniqueness can make it challenging to interpret the results of NMF, as different solutions may lead to different interpretations of the underlying factors. It can also make it difficult to compare results across different runs of the algorithm or when using different initialization strategies.\n",
    "> - Mitigation: Techniques like regularization, which adds constraints or penalties to the objective function, can help reduce the non-uniqueness issue by encouraging solutions with specific properties, such as sparsity or orthogonality.\n",
    "\n",
    "**Initialization Sensitivity:** The performance of NMF algorithms can be sensitive to the initial values of the factor matrices ($W$ and $H$). Different initialization strategies can lead to the algorithm converging to different local minima, resulting in different solutions.\n",
    "> - Implications: This sensitivity to initialization can make the results of NMF less reproducible and potentially lead to suboptimal solutions.\n",
    "> - Mitigation: Exploring different initialization methods, such as random initialization, NNDSVD (Non-negative Double Singular Value Decomposition), or using prior knowledge about the data, can help mitigate this issue and potentially improve the quality of the solutions.\n",
    "\n",
    "**Determining the Optimal Number of Factors:** Choosing the right number of factors (or components) is a crucial step in NMF. Too few factors may not capture all the important information in the data, leading to a loss of information and reduced accuracy. Too many factors can lead to overfitting, where the model captures noise or irrelevant patterns in the data, reducing its ability to generalize to new data.\n",
    "> - Implications: Selecting an inappropriate number of factors can significantly impact the performance and interpretability of the NMF model.\n",
    "> - Mitigation: Model selection techniques, such as cross-validation, silhouette analysis, or using domain expertise to assess the interpretability of the factors, can help guide the choice of the optimal number of factors.\n",
    "\n",
    "**Non-negative Data Requirement:** NMF is fundamentally designed to work with non-negative data. The algorithm assumes that the input data matrix ($V$) and the resulting factor matrices ($W$ and $H$) have only non-negative elements. This is because the non-negativity constraint is essential for ensuring the interpretability of the factors as additive combinations of original features.\n",
    "> - Implications: This limitation restricts the applicability of NMF to datasets where negative values are not meaningful or where they can be transformed into a non-negative representation.\n",
    "> - Mitigation: For data with negative values, techniques like shifting (adding a constant to all elements), scaling (multiplying by a positive constant), or using alternative matrix factorization methods that can handle mixed-sign data might be considered.\n",
    "\n",
    "**Interpretability Challenges:** While NMF is often considered more interpretable than other dimensionality reduction techniques like PCA, interpreting the resulting factors can still be subjective and require domain expertise. The factors represent latent features or patterns in the data, and their meaning may not always be immediately obvious.\n",
    "> - Implications: The interpretation of NMF factors requires careful analysis and consideration of the context of the data and the specific application.\n",
    "> - Mitigation: Techniques like visualizing the factor loadings, examining the top contributing features for each factor, and using domain knowledge to relate the factors to real-world concepts can aid in interpretation.\n",
    "\n",
    "**Computational Cost:** NMF algorithms can be computationally expensive, especially for large datasets or a high number of factors. The iterative nature of the algorithm and the need to update the factor matrices repeatedly can lead to significant computation time.\n",
    "> - Implications: The computational cost of NMF can be a limiting factor for large-scale applications or when real-time analysis is required.\n",
    "> - Mitigation: Using efficient algorithms, such as those based on sparse matrix operations, can help reduce the computational burden. Additionally, techniques like online NMF, which updates the factorization incrementally as new data arrives, can be more computationally tractable for streaming data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua4yw_fBIfDj"
   },
   "source": [
    "## **7. Comparing NMF with Other Matrix Decomposition Techniques**\n",
    "\n",
    "**Principal component analysis (PCA)** is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space by identifying the directions (principal components) along which the data varies the most. These principal components are orthogonal and capture the maximum variance in the data, allowing for data simplification, noise reduction, and feature extraction. PCA is often used for data visualization, noise reduction, and as a preprocessing step for machine learning. It's a versatile technique applicable to various data types, but the resulting components can be less interpretable. In summary, PCA finds the most important patterns in the data and represents them using a smaller number of variables (principal components) that are orthogonal. This simplifies the data while preserving its essential information.\n",
    "\n",
    "**Singular value decomposition (SVD)** is a more general-purpose matrix factorization technique that decomposes data into three matrices, capturing latent factors and their relationships. SVD is a matrix factorization technique that decomposes a matrix into three constituent matrices: $U$, $\\Sigma$ and $V^T$. These matrices capture the latent factors and relationships within the data, allowing for dimensionality reduction, noise reduction, and data compression. It's widely used in various applications, including recommendation systems, image compression, and noise reduction. However, like PCA, SVD components can be less interpretable. In summary, SVD breaks down a matrix into simpler components that reveal its underlying structure and relationships.\n",
    "\n",
    "**Independent component analysis (ICA)** is a computational technique that aims to separate a multivariate signal into additive subcomponents that are statistically independent and non-Gaussian. In summary, ICA aims to uncover the hidden sources or factors that contribute to a mixed signal. It's like \"unmixing\" a cocktail to identify its individual ingredients.\n",
    "\n",
    "**Non-negative matrix factorization (NMF)** specializes in decomposing non-negative data into two non-negative matrices, resulting in a parts-based representation. NMF decomposes non-negative data into two non-negative matrices, $W$ and $H$. This decomposition results in a parts-based representation, where the components (columns of $W$) represent individual parts or features of the original data, and their weights (rows of $H$) indicate their importance in each data point. This makes NMF particularly useful for tasks where interpretability and a parts-based decomposition are desired, such as image processing, text analysis, and topic modeling. NMF often produces sparse results, further enhancing interpretability. In essence, NMF breaks down data into additive, non-negative components that represent its essential parts or features.\n",
    "\n",
    "PCA, SVD, ICA, and NMF are all techniques used for dimensionality reduction and feature extraction, but they differ in their approaches and properties. Key differences are:\n",
    "\n",
    " - **Constraints:** PCA and SVD enforce orthogonality on their components, ICA enforces independence, and NMF enforces non-negativity.\n",
    " - **Interpretation:** PCA and SVD components can be difficult to interpret while ICA components represent underlying sources and NMF components are often more easily related to the original features.\n",
    " - **Sparsity:** PCA and SVD typically produce dense results while ICA can be sparse or dense and NMF often leads to sparse representations.\n",
    " - **Data types:** PCA, SVD, and ICA can be applied to any type of data, while NMF is limited to non-negative data.\n",
    "\n",
    "The choice between PCA, SVD, ICA, and NMF depends on the specific application and the desired properties. If capturing the maximum variance is the primary goal, PCA might be more appropriate. If a more general-purpose decomposition is needed, SVD might be more suitable. If blind source separation is desired, ICA would be the preferred technique. If interpretability and a parts-based representation are crucial, NMF is a good choice.\n",
    "\n",
    "In some cases, it can be beneficial to use these techniques in combination. For example, PCA or SVD could be used for initial dimensionality reduction, and then ICA or NMF could be applied to extract interpretable features from the reduced data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICTQzsoMLitT"
   },
   "source": [
    "## **8. Extensions of Non-negative Matrix Factorization (NMF)**\n",
    "\n",
    "While the standard NMF algorithm is widely used, various extensions and variations have been developed to address specific needs and improve performance. Here are a few notable ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6brXgqQ-JNW"
   },
   "source": [
    "### **8.1 Sparse NMF**\n",
    "\n",
    "Sparse non-negative matrix factorization (NMF) is a variation of the standard NMF algorithm that enforces sparsity on the factor matrices $W$ and $H$. This means that it encourages many elements of these matrices to be zero.\n",
    "\n",
    "**Reasoning:** Sparsity is often desirable in NMF for several reasons:\n",
    " - Interpretability: Sparse factor matrices are easier to interpret because they highlight the most important features and reduce the complexity of the model.\n",
    " - Feature Selection: Sparsity can act as a form of feature selection, identifying the most relevant features for representing the data.\n",
    " - Noise Reduction: By focusing on a smaller set of features, sparse NMF can help reduce the influence of noise in the data.\n",
    "\n",
    "**Methods:** Sparsity in NMF is typically achieved by adding sparsity-inducing regularization terms to the NMF objective function. These terms penalize non-zero elements in the factor matrices, encouraging them to become zero. Common regularization techniques include:\n",
    " - L1 regularization: Adds a penalty proportional to the sum of the absolute values of the elements in the factor matrices.\n",
    " - L2 regularization: Adds a penalty proportional to the sum of the squared values of the elements in the factor matrices.\n",
    " - Other sparsity constraints: Various other constraints can be used to enforce sparsity, such as limiting the number of non-zero elements in each row or column of the factor matrices.\n",
    "\n",
    "**Benefits:** Sparse NMF offers several benefits compared to standard NMF:\n",
    " - Improved interpretability: Sparse factor matrices are easier to understand and relate to the original features.\n",
    " - Better feature selection: Sparsity helps identify the most relevant features for representing the data.\n",
    " - Reduced noise: By focusing on a smaller set of features, sparse NMF can help reduce the influence of noise.\n",
    " - Enhanced generalization: Sparse models often generalize better to new data because they are less prone to overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F95QWy2MIFs"
   },
   "source": [
    "### **8.2 Constrained NMF**\n",
    "\n",
    "Constrained non-negative matrix factorization (NMF) is a variation of standard NMF where additional constraints are imposed on the factor matrices ($W$ and $H$) during the factorization process. These constraints are often based on prior knowledge about the data or specific properties desired in the resulting factors.\n",
    "\n",
    "**Reasoning:** Incorporating constraints allows for tailoring NMF to specific applications and data characteristics. This can improve the quality of the factorization, enhance interpretability, and ensure the results are more meaningful in the context of the problem.\n",
    "\n",
    "**Examples of Constraints:**\n",
    "\n",
    " - Sum-to-one Constraint: This constraint enforces that the elements in each row of $H$ (or each column of $W$) sum to one. It's commonly used in topic modeling, where each row of $H$ represents a document and the elements indicate the proportion of each topic in that document.\n",
    " - Orthogonality Constraint: This constraint requires that the columns of $W$ (or rows of $H$) be orthogonal to each other. It encourages the factors to be independent and represent distinct aspects of the data.\n",
    " - Sparsity Constraint: Similar to sparse NMF, this constraint promotes sparsity in the factor matrices, leading to more interpretable and noise-resistant results.\n",
    " - Domain-Specific Constraints: Constraints based on domain knowledge can be incorporated to guide the factorization. For example, in image processing, constraints could be used to ensure that the factors represent specific image features like edges or textures.\n",
    "\n",
    "**Benefits:** Constrained NMF offers several advantages:\n",
    "\n",
    " - Improved Factorization: Constraints can guide the factorization process toward more meaningful and relevant solutions.\n",
    " - Enhanced Interpretability: Constraints can make the resulting factors easier to interpret and relate to the original data.\n",
    " - Tailored Solutions: Constraints allow for adapting NMF to specific applications and data characteristics.\n",
    " - Control over Factor Properties: Constraints can be used to enforce desired properties in the factors, such as sparsity, orthogonality, or specific patterns.\n",
    "\n",
    "In essence, constrained NMF allows you to customize the NMF algorithm to your specific needs by incorporating prior knowledge or desired properties into the factorization process. This can lead to more insightful and relevant results compared to standard NMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zeu_qqOYN17X"
   },
   "source": [
    "### **8.3 Semi-NMF**\n",
    "\n",
    "Semi-NMF, or semi non-negative matrix factorization, is a variation of NMF where the non-negativity constraint is relaxed on one of the factor matrices (either $W$ or $H$). This means that one of the matrices can contain both positive and negative values while the other remains non-negative.\n",
    "\n",
    "**Reasoning:** The standard NMF algorithm requires all elements of the factor matrices to be non-negative. However, in some applications, negative values can be meaningful and provide valuable insights. For instance, in financial data analysis, negative returns are possible and should be considered in the factorization. Semi-NMF allows for the decomposition of such mixed-sign data while still preserving the interpretability and parts-based representation benefits of NMF.\n",
    "\n",
    "**How it Works:** Semi-NMF modifies the standard NMF objective function and update rules to accommodate the relaxation of the non-negativity constraint. The algorithm still aims to find two matrices ($W$ and $H$) whose product approximates the original data matrix ($V$), but one of the matrices can now have negative elements.\n",
    "\n",
    "**Benefits:**\n",
    " - Handling Mixed-Sign Data: Semi-NMF enables the decomposition of data with both positive and negative values, which is crucial for applications where negative values are meaningful.\n",
    " - Preserving Interpretability: While allowing negative values, semi-NMF still retains the interpretability and parts-based representation benefits of NMF.\n",
    " - Flexibility: It offers more flexibility compared to standard NMF by accommodating a wider range of data types.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    " - When using semi-NMF, it's crucial to choose which factor matrix ($W$ or $H$) should be allowed to have negative values based on the specific application and the interpretation of the factors.\n",
    " - The interpretation of the factors may differ slightly from standard NMF due to the presence of negative values. Careful consideration should be given to the meaning of negative values in the context of the problem.\n",
    "\n",
    "In summary, semi-NMF is a valuable extension of NMF that allows for the decomposition of mixed-sign data while still retaining many of the benefits of standard NMF. It offers more flexibility and can provide insights into data where negative values are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3MTsGCGQtEP"
   },
   "source": [
    "### **8.4 Convex NMF**\n",
    "\n",
    "Convex non-negative matrix factorization (convex NMF) is a variant of NMF where convexity constraints are imposed on the factorization. This means that the factor matrices ($W$ and $H$) are restricted to lie within a convex set.\n",
    "\n",
    "**Reasoning:** Incorporating convexity constraints in NMF can lead to several benefits:\n",
    "\n",
    " - Uniqueness: Convex NMF can help address the non-uniqueness issue inherent in standard NMF. By restricting the solution space to a convex set, it can reduce the number of possible solutions and potentially lead to a unique or more stable solution.\n",
    " - Improved Convergence: Convexity constraints can improve the convergence properties of the NMF algorithm, making it more likely to find a good solution efficiently.\n",
    " - Better Interpretability: In some cases, convexity constraints can lead to more interpretable factors by encouraging them to represent specific features or patterns in the data.\n",
    "\n",
    "**Methods:** Convexity constraints in NMF are typically enforced by restricting the factor matrices to lie within a convex set. This can be achieved through various methods, such as:\n",
    "\n",
    " - Convex Hull: The factor matrices are constrained to lie within the convex hull of a set of data points or features.\n",
    " - Polytope Constraints: The factor matrices are restricted to lie within a specific polytope defined by a set of linear inequalities.\n",
    " - Other Convex Constraints: Various other convex constraints can be used, such as restricting the factor matrices to be positive semidefinite or to have specific sparsity patterns.\n",
    "\n",
    "**Benefits:** Convex NMF offers several advantages compared to standard NMF:\n",
    "\n",
    " - Potential Uniqueness: It can help address the non-uniqueness issue of standard NMF, leading to more stable and reliable solutions.\n",
    " - Improved Convergence: Convexity constraints can improve the convergence properties of the NMF algorithm.\n",
    " - Enhanced Interpretability: In some cases, convexity constraints can lead to more interpretable factors.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    " - Choosing appropriate convex constraints is crucial for the success of convex NMF. The constraints should be relevant to the specific application and data characteristics.\n",
    " - Enforcing convexity constraints can increase the computational complexity of the NMF algorithm. However, efficient algorithms have been developed to address this challenge.\n",
    "\n",
    "In summary, convex NMF is a valuable variant of NMF that incorporates convexity constraints to improve the uniqueness, convergence, and interpretability of the factorization. It offers a powerful tool for various data analysis and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0OD_d6LLqQ1"
   },
   "source": [
    "### **8.5 Online-NMF:**\n",
    "\n",
    "Online non-negative matrix factorization (Online NMF) is a variant of NMF designed to handle streaming data, where new data points arrive continuously over time.\n",
    "\n",
    "**Reasoning:** Standard NMF assumes that the entire data matrix is available at once. However, in many real-world scenarios, data arrives in a streaming fashion, and it's not feasible to store and process the entire dataset at once. Online NMF addresses this challenge by updating the factorization incrementally as new data points become available.\n",
    "\n",
    "**How it Works:** Online NMF algorithms typically employ incremental update rules to incorporate new data points without recomputing the entire factorization. These update rules adjust the factor matrices ($W$ and $H$) based on the new data, gradually refining the factorization over time.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    " - Handling Streaming Data: Online NMF is well-suited for analyzing streaming data, where new data points arrive continuously.\n",
    " - Adaptability: It can adapt to changes in the data distribution over time, making it suitable for dynamic environments.\n",
    " - Efficiency: It avoids the need to store and process the entire dataset, making it more memory-efficient and computationally tractable for large datasets.\n",
    " - Real-time Analysis: Online NMF enables real-time analysis of streaming data, providing timely insights as new data becomes available.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    " - Choosing an appropriate update rule is crucial for the performance of online NMF. Different update rules have different properties and may be more suitable for specific types of data or applications.\n",
    " - The learning rate, which controls the step size of the updates, needs to be carefully tuned to balance stability and adaptability.\n",
    " - Online NMF may require more iterations or data points to converge compared to standard NMF, as the factorization is updated incrementally.\n",
    "\n",
    "In summary, online NMF is a valuable variant of NMF designed for handling streaming data. It offers adaptability, efficiency, and real-time analysis capabilities, making it suitable for various dynamic data environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_HH-mx02BNt"
   },
   "source": [
    "## **9. Application of NMF for Portfolio Diversification**\n",
    "\n",
    "In this section, we will explore a simplified example of portfolio diversification using the NMF technique. We will approach this by leveraging NMF's ability to extract meaningful features from financial data and providing a systematic way to diversify a portfolio based on underlying factors. This can be done with the following steps:\n",
    "\n",
    " - **Correlation Matrix:** We start with the correlation matrix of asset returns, which captures the relationships between different assets.\n",
    " - **NMF Decomposition:** NMF decomposes the correlation matrix into factor loadings ($W$) and factor scores ($H$). The factor loadings represent the contribution of each asset to each factor, while the factor scores represent the importance of each factor over time.\n",
    " - **Factor Interpretation:** By analyzing the factor loadings, we can identify groups of assets that exhibit similar behavior and understand the underlying factors that drive asset co-movements.\n",
    " - **Diversification:** We can diversify our portfolio by selecting assets that load highly on different factors. This reduces the overall portfolio risk by spreading investments across different sources of risk.\n",
    " - **Portfolio Construction:** Based on the factor analysis, we can determine the weights for each asset in our portfolio, ensuring non-negativity and summing to 1.\n",
    "\n",
    " In the following code snippet, we construct a simplified portfolio of data for five stocks: AAPL, MSFT, GOOG, AMZN, and TSLA. Then, we construct a `returns_df` DataFrame, containing daily returns for the selected assets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1729866972779,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "Tfi1Vv1n3Zub",
    "outputId": "45ed7352-5773-46a6-b2f1-629d55bbf158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  8 of 8 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>JPM</th>\n",
       "      <th>META</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>-0.012692</td>\n",
       "      <td>-0.016916</td>\n",
       "      <td>-0.004535</td>\n",
       "      <td>0.037910</td>\n",
       "      <td>-0.005937</td>\n",
       "      <td>-0.017147</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>-0.041833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>-0.026600</td>\n",
       "      <td>-0.018893</td>\n",
       "      <td>-0.046830</td>\n",
       "      <td>-0.018282</td>\n",
       "      <td>-0.036728</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>-0.057562</td>\n",
       "      <td>-0.053471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>-0.016693</td>\n",
       "      <td>-0.006711</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>-0.021523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>0.000988</td>\n",
       "      <td>-0.004288</td>\n",
       "      <td>-0.003973</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>-0.002015</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>-0.033040</td>\n",
       "      <td>-0.035447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-10</th>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>-0.011212</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.030342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>-0.002798</td>\n",
       "      <td>0.017425</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>-0.008671</td>\n",
       "      <td>-0.017551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>-0.013878</td>\n",
       "      <td>-0.025924</td>\n",
       "      <td>-0.020933</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>-0.009827</td>\n",
       "      <td>-0.007414</td>\n",
       "      <td>-0.071353</td>\n",
       "      <td>-0.114089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>-0.030685</td>\n",
       "      <td>-0.014692</td>\n",
       "      <td>-0.016718</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>-0.010780</td>\n",
       "      <td>-0.010255</td>\n",
       "      <td>-0.006020</td>\n",
       "      <td>0.033089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>0.028324</td>\n",
       "      <td>0.028844</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.040131</td>\n",
       "      <td>0.027630</td>\n",
       "      <td>0.040396</td>\n",
       "      <td>0.080827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>0.002469</td>\n",
       "      <td>-0.002138</td>\n",
       "      <td>-0.002473</td>\n",
       "      <td>0.006606</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>-0.004938</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          AAPL      AMZN      GOOG       JPM      META      MSFT   \n",
       "Date                                                                     \n",
       "2022-01-04 -0.012692 -0.016916 -0.004535  0.037910 -0.005937 -0.017147  \\\n",
       "2022-01-05 -0.026600 -0.018893 -0.046830 -0.018282 -0.036728 -0.038388   \n",
       "2022-01-06 -0.016693 -0.006711 -0.000745  0.010624  0.025573 -0.007902   \n",
       "2022-01-07  0.000988 -0.004288 -0.003973  0.009908 -0.002015  0.000510   \n",
       "2022-01-10  0.000116 -0.006570  0.011456  0.000957 -0.011212  0.000732   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-12-23 -0.002798  0.017425  0.017562  0.004745  0.007855  0.002267   \n",
       "2022-12-27 -0.013878 -0.025924 -0.020933  0.003504 -0.009827 -0.007414   \n",
       "2022-12-28 -0.030685 -0.014692 -0.016718  0.005465 -0.010780 -0.010255   \n",
       "2022-12-29  0.028324  0.028844  0.028799  0.005737  0.040131  0.027630   \n",
       "2022-12-30  0.002469 -0.002138 -0.002473  0.006606  0.000665 -0.004938   \n",
       "\n",
       "Ticker          NVDA      TSLA  \n",
       "Date                            \n",
       "2022-01-04 -0.027589 -0.041833  \n",
       "2022-01-05 -0.057562 -0.053471  \n",
       "2022-01-06  0.020794 -0.021523  \n",
       "2022-01-07 -0.033040 -0.035447  \n",
       "2022-01-10  0.005615  0.030342  \n",
       "...              ...       ...  \n",
       "2022-12-23 -0.008671 -0.017551  \n",
       "2022-12-27 -0.071353 -0.114089  \n",
       "2022-12-28 -0.006020  0.033089  \n",
       "2022-12-29  0.040396  0.080827  \n",
       "2022-12-30  0.000753  0.011164  \n",
       "\n",
       "[250 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download historical data for the tickers\n",
    "tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'META', 'JPM']\n",
    "data = yf.download(tickers, start='2022-01-01', end='2023-01-01')['Close']\n",
    "\n",
    "# 1. Load asset returns data\n",
    "returns_df = data.pct_change().dropna()\n",
    "returns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQr46t0O4drB"
   },
   "source": [
    "Now that we have the `returns_df` DataFrame, containing daily returns for the selected assets, we can use it in the NMF portfolio diversification code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1729866976265,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "tTenYqYA4kpw",
    "outputId": "06591648-30e2-4ebd-a0d4-c72d0da67771"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>JPM</th>\n",
       "      <th>META</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695904</td>\n",
       "      <td>0.790572</td>\n",
       "      <td>0.547907</td>\n",
       "      <td>0.592901</td>\n",
       "      <td>0.824901</td>\n",
       "      <td>0.763022</td>\n",
       "      <td>0.637219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.695904</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724022</td>\n",
       "      <td>0.501689</td>\n",
       "      <td>0.605782</td>\n",
       "      <td>0.741196</td>\n",
       "      <td>0.709078</td>\n",
       "      <td>0.591533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.790572</td>\n",
       "      <td>0.724022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.511192</td>\n",
       "      <td>0.681990</td>\n",
       "      <td>0.845283</td>\n",
       "      <td>0.767572</td>\n",
       "      <td>0.556651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPM</th>\n",
       "      <td>0.547907</td>\n",
       "      <td>0.501689</td>\n",
       "      <td>0.511192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393247</td>\n",
       "      <td>0.532507</td>\n",
       "      <td>0.528095</td>\n",
       "      <td>0.364937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>META</th>\n",
       "      <td>0.592901</td>\n",
       "      <td>0.605782</td>\n",
       "      <td>0.681990</td>\n",
       "      <td>0.393247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625860</td>\n",
       "      <td>0.607685</td>\n",
       "      <td>0.398646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.824901</td>\n",
       "      <td>0.741196</td>\n",
       "      <td>0.845283</td>\n",
       "      <td>0.532507</td>\n",
       "      <td>0.625860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787883</td>\n",
       "      <td>0.563946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>0.763022</td>\n",
       "      <td>0.709078</td>\n",
       "      <td>0.767572</td>\n",
       "      <td>0.528095</td>\n",
       "      <td>0.607685</td>\n",
       "      <td>0.787883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.680243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.637219</td>\n",
       "      <td>0.591533</td>\n",
       "      <td>0.556651</td>\n",
       "      <td>0.364937</td>\n",
       "      <td>0.398646</td>\n",
       "      <td>0.563946</td>\n",
       "      <td>0.680243</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker      AAPL      AMZN      GOOG       JPM      META      MSFT      NVDA   \n",
       "Ticker                                                                         \n",
       "AAPL    1.000000  0.695904  0.790572  0.547907  0.592901  0.824901  0.763022  \\\n",
       "AMZN    0.695904  1.000000  0.724022  0.501689  0.605782  0.741196  0.709078   \n",
       "GOOG    0.790572  0.724022  1.000000  0.511192  0.681990  0.845283  0.767572   \n",
       "JPM     0.547907  0.501689  0.511192  1.000000  0.393247  0.532507  0.528095   \n",
       "META    0.592901  0.605782  0.681990  0.393247  1.000000  0.625860  0.607685   \n",
       "MSFT    0.824901  0.741196  0.845283  0.532507  0.625860  1.000000  0.787883   \n",
       "NVDA    0.763022  0.709078  0.767572  0.528095  0.607685  0.787883  1.000000   \n",
       "TSLA    0.637219  0.591533  0.556651  0.364937  0.398646  0.563946  0.680243   \n",
       "\n",
       "Ticker      TSLA  \n",
       "Ticker            \n",
       "AAPL    0.637219  \n",
       "AMZN    0.591533  \n",
       "GOOG    0.556651  \n",
       "JPM     0.364937  \n",
       "META    0.398646  \n",
       "MSFT    0.563946  \n",
       "NVDA    0.680243  \n",
       "TSLA    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Calculate the correlation matrix\n",
    "correlation_matrix = returns_df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz3QvOkD_ULg"
   },
   "source": [
    "Non-negative matrix factorization (NMF) is fundamentally designed to work with non-negative matrices. This means that the input matrix ($V$) and the resulting factor matrices ($W$ and $H$) are expected to have only non-negative elements. However, in the specific context of portfolio diversification using NMF, the input matrix is often the correlation matrix of asset returns, which can contain negative values representing inverse relationships between assets.\n",
    "\n",
    "In our example, the correlation matrix has only positive values. But if required, we would consider the following adjustments in certain cases:\n",
    "\n",
    " - **Shifting:** we can shift the correlation matrix by adding 1 to all elements, ensuring all values are between 0 and 2. This might slightly alter the interpretation of the factor loadings but won't significantly affect the overall diversification strategy.\n",
    " - **Absolute Values:** Another option is to use the absolute values of the correlation coefficients, focusing on the strength of the relationships rather than their direction. This might be useful when we want to identify groups of assets with strong co-movements regardless of whether they are positive or negative.\n",
    "\n",
    "Next, we apply NMF to break down the correlation matrix into two smaller matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hLGZNrx7Avi9"
   },
   "outputs": [],
   "source": [
    "# 3. Apply NMF\n",
    "n_components = 5  # Number of factors to extract\n",
    "model = NMF(n_components=n_components, init='random', random_state=0)\n",
    "W = model.fit_transform(correlation_matrix)  # Factor loadings\n",
    "H = model.components_  # Factor scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i67ShyC4Bxd0"
   },
   "source": [
    "Here, we use the `NMF()` function from the `sklearn.decomposition` module in the scikit-learn library.\n",
    " - `n_components` specifies the number of (or components) to extract from the data. In the context of portfolio diversification, these factors represent underlying drivers of asset co-movements. In this specific case, setting n_components = 5 assumes that there are likely around 5 major underlying factors driving the co-movements of the assets in the portfolio. This is a reasonable starting point, but we should experiment with different values to find the optimal number for our specific data and investment goals. Choosing the right number of components is often an iterative process involving domain expertise, experimentation, and model evaluation.\n",
    " - `init='random'` specifies the initialization method for the factor matrices ($W$ and $H$). `'random'` initializes them with random non-negative values.\n",
    " - `random_state=0` sets a random seed for reproducibility.\n",
    "\n",
    "The next step involves analyzing the factor loadings ($W$) obtained from the NMF decomposition to understand the underlying factors driving asset co-movements and using this information to diversify the portfolio. We can access the factor loadings ($W$) using the variable `W` obtained in this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1729866982693,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "FKDvfIJvUgMV",
    "outputId": "4ee4693b-72f6-41fc-e576-41387c848ae2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.464385</td>\n",
       "      <td>0.268748</td>\n",
       "      <td>2.308236e-01</td>\n",
       "      <td>0.780432</td>\n",
       "      <td>0.083934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.667704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.201922e-01</td>\n",
       "      <td>0.276371</td>\n",
       "      <td>0.499709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.458127</td>\n",
       "      <td>0.178794</td>\n",
       "      <td>1.562600e-01</td>\n",
       "      <td>0.803558</td>\n",
       "      <td>0.246558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPM</th>\n",
       "      <td>0.251893</td>\n",
       "      <td>0.864869</td>\n",
       "      <td>1.186287e-07</td>\n",
       "      <td>0.396025</td>\n",
       "      <td>0.208778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>META</th>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.265370</td>\n",
       "      <td>2.902260e-01</td>\n",
       "      <td>0.680850</td>\n",
       "      <td>0.639629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.550907</td>\n",
       "      <td>0.165174</td>\n",
       "      <td>1.144748e-01</td>\n",
       "      <td>0.780761</td>\n",
       "      <td>0.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>0.406845</td>\n",
       "      <td>0.266953</td>\n",
       "      <td>3.439410e-01</td>\n",
       "      <td>0.672620</td>\n",
       "      <td>0.173928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.310214</td>\n",
       "      <td>0.169142</td>\n",
       "      <td>6.282630e-01</td>\n",
       "      <td>0.296562</td>\n",
       "      <td>0.046809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1             2         3         4\n",
       "Ticker                                                      \n",
       "AAPL    0.464385  0.268748  2.308236e-01  0.780432  0.083934\n",
       "AMZN    0.667704  0.000000  1.201922e-01  0.276371  0.499709\n",
       "GOOG    0.458127  0.178794  1.562600e-01  0.803558  0.246558\n",
       "JPM     0.251893  0.864869  1.186287e-07  0.396025  0.208778\n",
       "META    0.003509  0.265370  2.902260e-01  0.680850  0.639629\n",
       "MSFT    0.550907  0.165174  1.144748e-01  0.780761  0.174300\n",
       "NVDA    0.406845  0.266953  3.439410e-01  0.672620  0.173928\n",
       "TSLA    0.310214  0.169142  6.282630e-01  0.296562  0.046809"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert W to a pandas DataFrame for easier analysis\n",
    "W_df = pd.DataFrame(W, index=returns_df.columns)\n",
    "W_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mWu65pkUlHC"
   },
   "source": [
    "The factor loadings matrix ($W$) provides insights into how much each asset contributes to each factor.\n",
    "\n",
    " - Rows: Represent the assets in your portfolio.\n",
    " - Columns: Represent the extracted factors.\n",
    " - Values: Indicate the strength of the relationship between an asset and a factor. Higher values suggest a stronger contribution of the asset to that factor.\n",
    "\n",
    "To interpret the factors, we need to examine the assets that load highly on each factor. For each factor, we need to identify the assets with the highest loadings. These assets are most strongly associated with that factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1729866986553,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "ixCqMNM_T_BH",
    "outputId": "52f29a84-9197-43fa-d475-c841a70e9d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Factor 1:\n",
      "Ticker\n",
      "AMZN    0.667704\n",
      "MSFT    0.550907\n",
      "AAPL    0.464385\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Factor 2:\n",
      "Ticker\n",
      "JPM     0.864869\n",
      "AAPL    0.268748\n",
      "NVDA    0.266953\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Factor 3:\n",
      "Ticker\n",
      "TSLA    0.628263\n",
      "NVDA    0.343941\n",
      "META    0.290226\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Factor 4:\n",
      "Ticker\n",
      "GOOG    0.803558\n",
      "MSFT    0.780761\n",
      "AAPL    0.780432\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Factor 5:\n",
      "Ticker\n",
      "META    0.639629\n",
      "AMZN    0.499709\n",
      "GOOG    0.246558\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Identify top contributing assets for each factor\n",
    "n_top_assets = 3\n",
    "for factor_num in range(W_df.shape[1]):\n",
    "    print(f\"\\nFactor {factor_num + 1}:\")\n",
    "    top_assets = W_df.iloc[:, factor_num].nlargest(n_top_assets)\n",
    "    print(top_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djgKBCDCVM3B"
   },
   "source": [
    "This output shows the top 3 contributing assets for each factor and their loadings. Remember that we can adjust `n_top_assets` and the interpretation of factors based on our specific data and investment goals. For now, we keep the top 3 contributing assets. We can now use this information to interpret the factors and develop a diversification strategy. Based on the top contributing assets for each factor, we can attempt to interpret their economic or financial meaning:\n",
    "\n",
    " - **Factor 1:** Dominated by AMZN, MSFT, and AAPL. This factor could be interpreted as representing **Large-Cap Tech or Growth Stocks**. These companies are often associated with innovation, technological advancements, and high growth potential.\n",
    " - **Factor 2:** Primarily driven by JPM, with moderate contributions from AAPL and NVDA. This factor might represent **Financials or Value Stocks**. JPM is a major financial institution, and this factor could reflect the performance of the financial sector or companies with more established value characteristics.\n",
    " - **Factor 3:** Led by TSLA and NVDA, with some influence from META. This factor could be interpreted as **Electric Vehicles or Disruptive Technology**. TSLA is a leading electric vehicle manufacturer, and NVDA is a major semiconductor company, both associated with disruptive technologies.\n",
    " - **Factor 4:** Strongly influenced by GOOG, MSFT, and AAPL. This factor seems to represent **Big Tech or Market Leaders**. These companies are dominant players in the technology sector and have a significant impact on the overall market. It might overlap somewhat with Factor 1, but with a broader focus on market leadership.\n",
    " - **Factor 5:** Predominantly driven by META, AMZN, and GOOG. This factor could be interpreted as **Social Media and E-commerce**. META is a major social media company, AMZN is a dominant e-commerce player, and GOOG has a strong presence in both areas.\n",
    "Diversification Strategy\n",
    "\n",
    "Based on this factor interpretation, a diversified portfolio could be constructed by selecting assets that load highly on different factors. For example, we could consider:\n",
    "\n",
    " - **Large-Cap Tech/Growth:** AMZN or MSFT (Factor 1)\n",
    " - **Financials/Value:** JPM (Factor 2)\n",
    " - **Electric Vehicles/Disruptive Tech:** TSLA or NVDA (Factor 3)\n",
    " - **Big Tech/Market Leaders:** GOOG or AAPL (Factor 4)\n",
    " - **Social Media/E-commerce:** META (Factor 5)\n",
    "\n",
    "By diversifying across these factors, we would be spreading investments across different sectors and risk profiles, potentially reducing overall portfolio risk and enhancing returns.\n",
    "\n",
    "This was just a simplified interpretation for this demonstration. Further analysis might be needed to refine the factor definitions. And we also need to remember that the optimal diversification strategy depends on individual investment goals, risk tolerance, and time horizon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMe4u5bOmpV1"
   },
   "source": [
    "## **10. Conclusion**\n",
    "\n",
    "In this lesson, we learned about Non-negative Matrix Factorization (NMF), a dimensionality reduction technique with applications in financial engineering. We discussed the definition, properties, and variations of NMF, comparing it to other methods like PCA, SVD, and ICA. The lesson then focused on NMF's applications in finance, including portfolio diversification, risk management, and sentiment analysis. It demonstrated a simplified but practical example of using NMF for portfolio diversification with stock market data, showing how to extract and interpret factors to build a diversified portfolio. Overall, the lesson highlighted NMF as a powerful and interpretable tool for analyzing financial data and making informed investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KvTOJQ-GFrc1"
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
