{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQfsxvu2MrAc"
   },
   "source": [
    "MODULE 7 | LESSON 3\n",
    "\n",
    "\n",
    "# **Numerical Methods for Data Preparation and Statistical Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSW9-996Miww"
   },
   "source": [
    "\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |45 minutes|\n",
    "|**Prior Knowledge** |Basic understanding of quantitative areas: linear algebra, calculus, statistical inference, and regression|\n",
    "|**Keywords** |Imputation, Calibration, Feature Extraction|\n",
    "|  |  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tsZsezGth8R"
   },
   "source": [
    "## **1. Data Cleaning**\n",
    "\n",
    "In this first section, we review two groups of numerical methods: those for imputing and those for excluding.\n",
    "\n",
    "### **1.2 Imputing: Filling in Missing Values**\n",
    "\n",
    "When we are given data, we may notice that some of the values are missing. Imputation is the name given to the action of providing missing values. We will learn various methods to fill in missing data, some of which are quite simple while others will instead involve a fair amount of complexity.\n",
    "\n",
    "On the simple side, we have methods that simply use the last known value (e.g., fill forward). We could also replace a missing value by looking at its neighbors and performing some degree of interpolation: linear, quadratic, cubic, quartic, splines, etc. We’ll also use resampling methods to handle missing or censored data. As we build our stochastic modeling skills, we’ll look at the expectation maximization algorithm and the Brownian bridge approach.\n",
    "<br>\n",
    "One data-imputing application we learned in this course (Module 1, Lesson 4) is to use a cubic spline equation to fit a yield curve. In the section Cubic Spline Fitting of Yield Curve, we use a cubic spline equation to estimate missing yield information from the yield dataset. With this method, we are able to fit a smooth and piecewise yield curve for the following yield curve analysis.\n",
    "\n",
    "### **1.2 Excluding: Outlier Detection**\n",
    "The previous subsection discussed how to handle missing values. We will also develop skills in the opposite direction: taking existing data values and excluding them, a process known as outlier detection. We will learn various methods for labeling and discarding outliers. Outliers are observations that cause the model’s results to be extremely sensitive to those points. To understand outliers, we’ll need to use both exploratory data analysis (prior to modeling) and diagnostic plots (post modeling). Exploratory data analysis is early visualization of raw data. Diagnostic plots show the sensitivity of data points to model parameter values and estimation.\n",
    "\n",
    "Under the same umbrella of excluding, we’ll look at dropping not just rows but also entire variables. Indeed, some of the methods we'll use take care of this automatically. For example, the Lasso method drops variables based on a parameter. Discarding the most egregious outliers could result in getting different parameters, which we can interpret and apply very differently. However, in our taxonomy, we’ll consider feature extraction a separate category.\n",
    "<br>\n",
    "In the next course, Financial Econometrics, we will dive deeper into the impact of outliers on regression models, formal methods to detect them, and specialized regression methods to handle outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7oGgqS8DhzS"
   },
   "source": [
    "## **2. Data Engineering**\n",
    "\n",
    "### **2.1 Transforming and Normalizing**\n",
    "Sometimes, only a simple transformation is needed. A transform could be as simple as a conversion (e.g., degrees Fahrenheit to degrees Celsius), where the conversion factor never changes. Other times, the conversion factor does change: for example, converting Japanese yen to Nigerian naira.\n",
    "\n",
    "What are some of the transforms we’ll do? You’ve already transformed financial data series using logs. You may transform prices into volume-weighted average prices or VWAPs. We’ll do this in the next lesson. You can transform prices to returns. You can transform financial time series using lags so that a series can be regressed on itself or lagged versions of other series. Some transformations are more complex, like a Fourier transformation, which will prove to be useful in option pricing.\n",
    "\n",
    "Sometimes, we need to transform data to make it more usable through better-behaved measures such as having a zero-mean or a unit variance. Another set of transformations, called normalization, can have multiple meanings that depend on the context. For example, this includes the transformations that relate to making data more Gaussian: symmetric and bell shaped. In other cases, it is used interchangeably with standardization, where we subtract the mean and divide by the standard deviation. Lastly, it is used when we want to eliminate the effects of external factors, which is what we do when we detrend a time series. The purpose of these normalizations is to facilitate the analysis of the features of a dataset. Almost every example we’ll do has one type of transformation. Over time, these will become second nature.\n",
    "\n",
    "In this course, you have already transformed daily closing prices into logarithmic or percent returns. You have also seen that daily closing prices themselves are a transformation of raw tick-by-tick data. Furthermore, we computed the adjusted-close price by transforming the closing prices using data for splits and dividends. So, one can see that even the financial data readily available from multiple vendors are already heavily cleaned and transformed. In the rest of the Financial Engineering course, you will dive deep into multiple transformations such as standardization for comparing different assets on the same scale, Fourier transformations for option pricing, and many more. You'll learn not just how to apply these transformations but also understand their mathematical foundations and, most importantly, when and why to use them in real-world scenarios.\n",
    "\n",
    "\n",
    "\n",
    "### **2.2 Factoring and Extracting**\n",
    "Factoring and feature extraction are another way of reorganizing data. Factoring is a difficult process because it involves both identification of factors and their measurement. There are a bewildering number of variations of forming factors. Principal components can be a first step, but these components are often not very intuitive. So sometimes, we simplify the problem by using classical features engineering to shape data that can be used in many ML algorithms. Less straightforward methods require a serious amount of subject matter expertise to define relevant factors. Within the professional world, you will encounter factor models, such as the FICO scores in credit risk. On the portfolio management and risk management sides, you will encounter commercially available factor models like the MSCI Barra Factor Index. There are different versions for different regions of the world.\n",
    "\n",
    "Incidentally, the British refer to factor modeling as latent variable analysis: a good name to indicate that the variables need to be extracted. An example of a latent variable comes from the book (and Hollywood movie) *Moneyball*. For many years, it was thought that the most important metric of a baseball player was the number of hits or homeruns. However, a less obvious but more valuable factor is the player's percentage of getting on base. Aside from getting hits, there are two other ways to get on base. One way is to walk. The other is to get struck by a pitch. Whether the batter gets a hit, a walk, or hit by a pitch, the batter winds up on base and can potentially score, which is what wins games. The percent-on-base metric proved to be more important for winning games than the more easily observed number of hits. The \"percent-on-base\" factor is an example of a latent variable. One has to form a ratio: total times the batter gets on base from any of the three methods divided by the total number of times the batter has an official plate appearance.\n",
    "\n",
    "One of the complexities of finding factors is overcoming biases. After writing *Moneyball*, Michael Lewis wrote *The Undoing Project*. In that book, he explains why walks were not considered as valuable as hits. In order for a batter to draw a walk, he must avoid swinging his bat four times when the pitches are not in the strike zone. From one perspective, it appears that the batter is taking no action. On the other hand, when the batter swings and connects the ball, there is the crack of the bat, running by both the batter and the players in the field, and applause by the fans—all reactions that give the illusion that the action is more valuable than the motionless stance of a walk, which typically draws less applause. However, both a single and a walk have the same outcome: the batter proceeds to first base. The cognitive bias is one of discounting a result because it was achieved by little action—not swinging. But the batter is exhibiting excellent skill with a good eye on a fast-moving ball and restraint from chasing bad pitches.\n",
    "\n",
    "Think back to Module 1. You've already witnessed the power of creating factors when you \"factored out\" the level and slope of the yield curve using the first and second principal components of the interest rates. In Financial Engineering, you'll dive deeper into advanced factoring techniques like factor analysis (FA), which goes beyond PCA by attempting to explain the correlations among variables through a smaller set of **interpretable** factors. These techniques are not just academic exercises; they're used by industry leaders to build advanced factor models for portfolio management and risk assessment.\n",
    "\n",
    "Being able to extract features and define factors is one of the most valuable skills you can build as a financial engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx_i0p7WD8EK"
   },
   "source": [
    "## **3. Statistical Analysis**\n",
    "\n",
    "### **3.1 Inferencing and Estimating: Parameter Evaluation**\n",
    "Flush with data, we will be well equipped to work on inference problems. Given samples of data, how do we best estimate the parameters of the population? What is a population’s mean? Median? Standard deviation? First percentile? Skewness? Kurtosis? Correlation to another variable?\n",
    "\n",
    "Suppose you assume the yield curve follows a polynomial model given by Nelson Siegel. Then, you can estimate the NS parameters from the data. This exercise is one you performed back in Module 1. Thanks to advances in computational power and contributions from fields outside statistics, there are many specific methods with which to perform inference. These methods typically fall into one of two major classes: frequentist approaches and Bayesian approaches. Within these are many subcategories, like nonparametric and resampling. Of course, regression problems involve an estimation of parameters, but we’ll consider this category as the estimation of parameters outside of regressions. For example, if we fit a Student’s t-copula to model the returns of two corporate bonds in a portfolio, how can we infer their correlation?\n",
    "\n",
    "### **3.2 Simulating and Randomizing**\n",
    "This is one example where we do not need any data. Simulations provide an opportunity to create randomized data according to a known distribution. Generating the data is useful when we have new securities or histories that are too short or when we would like to test how our model works under known distributions. Monte Carlo (MC) simulations are the pre-eminent simulation in finance. We will use MC simulation to generate stock prices, returns, volatilities, interest rates, and many other variables. Depending on the complexity of the distribution, these simulations can provide a deep understanding of how models work with the specified type of data. Many methods in derivatives rely on MC simulations whenever simple closed-form analytic solutions do not exist. Recall that in Module 3, Lesson 2, we performed a Monte Carlo simulation to compute the Value at Risk from a Gaussian distribution.\n",
    "\n",
    "If we want to impose a distribution on a system, then MC simulation tends to work very well. Other times, we may have a complex system in which it is implausible to run an MC simulation. In these cases, instead of working top down, we may work bottom up. One example of this approach is an agent-based model (ABM). Agent-based simulations are a different type of simulation that can model complex systems. Rather than fitting a distribution to a complex system with many interacting parts, an agent-based model (ABM) can be used to model the behavior of individual agents that then interact in complex ways. From this system, properties emerge that can create outcomes like those observed in the real world. Emergence is one of the key properties of ABM. We’ll look at an ABM of a marketplace that replicates the Flash Crash.\n",
    "\n",
    "In general, we’ll use simulations for back-testing models; pricing and hedging derivatives; optimizing portfolios and trading strategies; and stress-testing risk models. In some cases, we’ll generate multiple series of random variables, which preserve a given correlation to them. The ability to create random numbers is key throughout the entire program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRo2wFOGtwFg"
   },
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "In this lesson and the next one, we are gathering and organizing the numerical methods you have encountered in this course and will encounter throughout the program. In this lesson, we focused on data cleaning, data engineering, and statistical analysis. Data cleaning identified imputing missing values and excluding outliers. Data engineering focused on transforming and normalizing data, as well as methods for factoring and conditioning data. Statistical analysis covered inference and estimation, as well as simulating and randomizing. In the next lesson, we will complete the second half of numerical methods, including core modeling, model refinement, and finding optimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KvTOJQ-GFrc1"
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
