{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oVKJREwBGMx"
   },
   "source": [
    "# GOVERNMENT BOND YIELD BIVARIATE ANALYSIS AND YIELD CURVE ANALYSIS\n",
    "MODULE 1 | LESSON 4\n",
    "\n",
    "\n",
    "---\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** 60 minutes |   |\n",
    "|**Prior Knowledge** U.S. Treasury Bonds, Yield Curve, Linear Algebra, Basic Python |   |\n",
    "|**Keywords** Bond price-yield curve, Risk free interest rate, Nelson Siegel, Cubic Spline|  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8kG4QIGBpRo"
   },
   "source": [
    "*In the previous lesson, we introduced U.S. Treasury price yield curve and discussed the shape of the curve. We also introduced the methods to fit U.S. Treasury curves using polynomial fitting techniques. In this lesson, we will continue to learn new analytical methods to understand U.S. Treasury yields. First, we will explore bivariate relationships of different yields, including correlation and covariance. Then, we will learn a new technique to extract key features of the U.S. Treasury yield curve to analyze the yield curve's behavior.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9cZwuI9DfdS"
   },
   "source": [
    "## **1. Bivariate Analysis**\n",
    "**Bivariate analysis** is a collection of methods to analyze the relationship between two variables. In finance, we are particularly interested in understanding how two variables interact with each other. In this lesson, we'll look at the yields of different U.S. Treasury bonds and how they move in relation to each other. The first bivariate analysis tool we will learn is covariance.\n",
    "<br>\n",
    "<br>\n",
    "### **1.1 Covariance**\n",
    "**Covariance** is a metric to measure how two variables move together. Specifically, covariance calculates the amount of movement the two variables exhibit. Here is the covariance formula:\n",
    "<br>\n",
    "$$Cov(ùëã,ùëå)=ùê∏[(ùëã‚àíùê∏[ùëã])(ùëå‚àíùê∏[ùëå])]$$\n",
    "<br>\n",
    "Here are some of the properties of covariance:\n",
    "1. If the covariance has a positive sign, it means the two variables move in the same direction.\n",
    "2. If the covariance has a negative sign, the two variables move in opposite directions.\n",
    "3. If the covariance is 0, the two variables are linearly uncorrelated (uncorrelated).\n",
    "\n",
    "The higher the absolute value of the covariance of the two variables, the stronger the (positive or negative) relationship the two variables have. Let's pull some U.S. Treasury yield data to demonstrate covariance between two different yields.\n",
    "<br>\n",
    "\n",
    "A covariance matrix is a bivariate analysis measure for two variables. However, when we have several variables and want to know the pairwise relationships of these variables, we'll need a covariance matrix. A **covariance matrix** is a square matrix that represents the pairwise covariances between multiple variables in a dataset. Now, let's pull U.S. Treasury yield data and investigate their covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0zyKOa8oIUFx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HtMR4LigIVTA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the FRED API with your key\n",
    "fred = Fred(api_key='9054697d5de01c254dc114e6da492bc7') # Replace my APIKEY with \"YOUR_API_KEY\"\n",
    "\n",
    "# List of Treasury yield series IDs\n",
    "series_ids = ['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3', 'DGS5', \\\n",
    "              'DGS7', 'DGS10', 'DGS20', 'DGS30']\n",
    "\n",
    "# Function to get data for a single series\n",
    "def get_yield_data(series_id):\n",
    "    data = fred.get_series(series_id, observation_start=\"1975-01-01\", observation_end=\"2024-05-03\")\n",
    "    return data\n",
    "\n",
    "# Get data for all series\n",
    "yields_dict = {series_id: get_yield_data(series_id) for series_id in series_ids}\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "yields = pd.DataFrame(yields_dict)\n",
    "\n",
    "# Rename columns for clarity\n",
    "yields.columns = ['1 Month', '3 Month', '6 Month', '1 Year', '2 Year', '3 Year', '5 Year', \\\n",
    "                  '7 Year', '10 Year', '20 Year', '30 Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BiOadjQpIqxa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make datetime as the index\n",
    "yields.index = pd.to_datetime(yields.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m3dKd4xse4K0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop NaN in the dataset\n",
    "yields = yields.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1727915159062,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "gluiXc8CJDqR",
    "outputId": "e6f8e3d0-c70a-45a4-9842-2a05c9c4bc5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate covariance matrix for US Treasury yields in the dataset\n",
    "covariance_matrix = yields.cov()\n",
    "print(\"Covariance Matrix:\")\n",
    "print(covariance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3l7RC-ZMX1i"
   },
   "source": [
    "From the above covariance matrix for U.S. Treasury yields, we can see all pairwise yields have positive covariances. It means when the yield of one maturity increases, the other yields will also increase. We can visualize the covariance matrix with a heatmap in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 1657,
     "status": "ok",
     "timestamp": 1727915167672,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "D02NoAVoNdMF",
    "outputId": "139eff66-e018-4b8e-c305-28736710f1e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make a heatmap for covariance matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(covariance_matrix, annot=True, cmap='coolwarm', fmt=\".1f\")\n",
    "plt.title('Covariance Heat Map of Treasury Bond Yields')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehnc8KQlOdnz"
   },
   "source": [
    "From the above covariance matrix heatmap, we can better read the differences of the covariance numbers. However, one downside of covariance is its value changes when the scales of two variables change. We cannot just compare different covariances for different pairwise variables to evaluate how close they move together. Because of this issue, we use correlation more often in data analysis.\n",
    "<br>\n",
    "<br>\n",
    "### **1.2 Correlation**\n",
    "Correlation is also a metric to measure the co-movement of the two variables. However, it eliminates the scale issue mentioned above by dividing covariance with the square root of the multiplication of the two variables' variances. Here is the correlation math formula:\n",
    "<br>\n",
    "<br>\n",
    "$$Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)*Var(Y)}}$$<br>\n",
    "where $Cov(X,Y)$ is the covariance of $X$ and $Y$, $Var(X)$ and $Var(Y)$ are variances of $X$ and $Y$.\n",
    "<br>\n",
    "<br>\n",
    "Here are some properties of correlation:\n",
    "\n",
    "1. Unlike covariance, the value of correlation is limited between ‚àí1 and 1.\n",
    "2. If the correlation of two variables is greater than 0, the two variables are positively correlated.\n",
    "3. If the correlation of two variables is less than 0, the two variables are negatively correlated.\n",
    "4. If two variables are perfectly positively correlated, the correlation will be 1.\n",
    "5. If two variables are perfectly negative correlated, the correlation will be ‚àí1.\n",
    "6. If the correlation is 0, the two variables are linearly uncorrelated.\n",
    "\n",
    "Like with a covariance matrix, if we have several variables, we can use a **correlation matrix** to present correlation metrics for pairwise variables. Let's calculate the correlation matrix for our U.S. Treasury yield data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1727915176309,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "0w9b6qX2S_NZ",
    "outputId": "3f4b959b-6891-4046-eb99-0e3748c0a2da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for US Treasury yields in the dataset\n",
    "correlation_matrix = yields.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8FCYxq2TWVL"
   },
   "source": [
    "Next, let's make a heatmap for the correlation matrix to better read the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1727915182931,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "LNzZMr-oTdby",
    "outputId": "c2e671d9-407e-4b38-9dcb-a4b45825ba54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a heatmap for correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heat Map of Treasury Bond Yields')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVVxdCGeTuKj"
   },
   "source": [
    "From the above correlation matrix heatmap for U.S. Treasury yields, we can compare correlations among different pairwise yields. First, we can see the correlations on the diagonal cells are all 1s. It is easy to understand because all yields move perfectly in the same direction with themselves. Secondly, another interesting observation is that two yields that have close maturities have higher correlations. As the difference of the maturities of two yields widen, the correlation of the two yields gets lower.\n",
    "<br>\n",
    "<br>\n",
    "Due to its easy-to-understand property, a correlation metric is a commonly used and crucial measurement for bivariate analysis. Understanding the correlation between different Treasury bond yields is crucial for risk management, portfolio construction, and understanding market dynamics. It helps quantify how different parts of the yield curve move in relation to each other, which is essential for strategies like yield curve trades and immunization in fixed-income portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPMoDnmHWTXH"
   },
   "source": [
    "### **2. Feature Extractions and Principal Component Analysis**\n",
    "In this section, we are going to introduce a method to extract key features from a dataset. It is called **principal component analysis (PCA)**. Oftentimes, we try to discover key common factors or features that can explain the movement of all variables in a dataset. If we can find these key common factors that can represent most of the data variation in the dataset, we won't need to use the whole dataset for analysis. Instead, we can focus on analyzing these key factors. This is a data dimension reduction technique. It is very crucial that it can reduce the size of the dataset to make a lot of algorithms run more efficiently.  \n",
    "<br>\n",
    "In this section, we will continue to use the U.S. Treasury yield dataset to demonstrate this technique. We will show a step-by-step process for conducting PCA. Our first step is to standardize the scales of variables in our dataset.\n",
    "<br>\n",
    "<br>\n",
    "### **2.1 Standardizing Variables**\n",
    "**Standardizing (or normalizing)** a variable is a common statistical technique used to transform a variable into a standard scale. It converts a variable so that it has a mean of 0 and a standard deviation of 1. Here is the formula to standardize a variable:\n",
    "\n",
    "$$Z=\\frac{X-\\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "<br>\n",
    "$Z$ = standardized variable with mean = 0 and standard deviation = 1\n",
    "<br>\n",
    "$X$ = original variable\n",
    "<br>\n",
    "$\\mu$ = mean of the original variable\n",
    "<br>\n",
    "$\\sigma$ = standard deviation of the original variable\n",
    "\n",
    "Here are the benefits of standardizing variables:\n",
    "1. Making all variables comparable on the same scale\n",
    "2. Facilitating calculation of the other analyses requiring same-scale variables\n",
    "\n",
    "Now let's standardize the U.S. Treasury yield dataset. We first need to calculate the mean and standard deviation of each yield. Then, we will apply the above formula to our dataset to create a standardized (normalized) dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1727915188740,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "N88nHbk0eJ9r",
    "outputId": "e2075d00-fffc-41c5-8148-87fe26ebb1f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate means for all yields in the dataset\n",
    "yield_means = yields.mean()\n",
    "print(\"Yield Means:\")\n",
    "print(yield_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1727915193227,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "QUTB31q3ec_M",
    "outputId": "d5bca7e6-ce57-428f-a056-4b80ebbbb96a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate standard deviations for all yields in the dataset\n",
    "yield_stds = yields.std()\n",
    "print(\"Yield Standard Deviations:\")\n",
    "print(yield_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1727915194554,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "s_7MgFNHgBuN",
    "outputId": "6ad41bba-ef63-474e-ebf3-20cbd2aef839",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now create a standardized US Treasury yield dataset\n",
    "standardized_data = (yields - yield_means) / yield_stds\n",
    "print(\"Standardized Yield (first 5 rows):\")\n",
    "print(standardized_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmk_f0FIgsLM"
   },
   "source": [
    "Usually, preparing a standardized dataset is part of the data preparation step before running the analysis. Make sure to standardize your dataset if the analysis you are going to run requires variables to be on the same scale.\n",
    "<br>\n",
    "Before moving on to the next step of the PCA, we need to learn new tools from linear algebra. In the next two sections, we will introduce eigenvectors and eigenvalues.\n",
    "<br>\n",
    "### **2.2 Eigenvectors and Eigenvalues**\n",
    "In this section, we will learn the definitions of eigenvectors and eigenvalues and how to calculate them. Eigenanalysis is a very important linear algebra method in finance. Please review the required reading for this lesson to understand this topic. In the next section, we will use visualization to enhance our understanding of eigenvectors and eigenvalues.\n",
    "<br>\n",
    "### **2.3 Visualization of Eigenvectors and Eigenvalues**\n",
    "In this section, we will use graphs to explain the concepts of eigenvectors and eigenvalues.\n",
    "<br>\n",
    "\n",
    "Vectors in a two-dimensional coordinate system are described by their magnitude and direction. A linear transformation occurs when we multiply a vector by a matrix. The transformation can change both the vector's magnitude and direction.\n",
    "However, there are certain unique vectors that will maintain their direction or just switch to the opposite direction when a linear transformation happens. The only change is the magnitude of these vectors. These special vectors are called eigenvectors. Let's first revisit the equation between eigenvectors and eigenvalues from the last equation:\n",
    "<br>\n",
    "$$Ax=\\lambda x$$\n",
    "<br>\n",
    "where:\n",
    "<br>\n",
    "$A$ is the linear transformation matrix\n",
    "<br>\n",
    "$x$ is eigenvector\n",
    "<br>\n",
    "$\\lambda$ is the magnitude metric or eigenvalue\n",
    "<br>\n",
    "<br>\n",
    "Let's first look at the left side of the equation. It corresponds to the linear transformation of a vector we described above. On the right side, the vector multiplies with a scalar. When the left-hand side of the equation equals the right-hand side of the equation, it means the linear transformation of the vector equals a magnitude change of the same vector. Based on what we described about an eigenvector, the vector in this equation is an eigenvector. Figure 1 below demonstrates this concept visually.\n",
    "<br>\n",
    "<br>\n",
    "**Figure 1: Visual Demonstration of Eigenvectors and Eigenvalues**\n",
    "\n",
    "![Graph showing a linear transformation of an (x, y) system with a circle transformed into an ellipse, highlighting eigenvector v_1 and its scaled version \\lambda v_1](images/FD_M1_L4_fig1.jpg)\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA8BdnWONq8W"
   },
   "source": [
    "From the above figure, we can see the purple eigenvector preserves its direction after the linear transformation, which is shown by the orange vector. The transformation merely stretches or compresses the eigenvector, without changing its direction.\n",
    "<br>\n",
    "<br>\n",
    "The eigenvalue associated with an eigenvector is the number by which the eigenvector is scaled during transformation.\n",
    "   - If the eigenvalue is positive, the eigenvector is stretched in its original direction.\n",
    "   - If negative, the eigenvector is stretched in the opposite direction.\n",
    "   - If the eigenvalue is 1, the eigenvector remains unchanged.\n",
    "   - If 0, the eigenvector is collapsed to a point.\n",
    "When representing eigenvalues as vector lengths, the length of each eigenvector arrow can be scaled to represent its corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i1n1pkTjrjQ"
   },
   "source": [
    "### **2.4 Derive Principal Components**\n",
    "The next step of PCA is to find the covariance matrix of the standardized dataset. For a standardized dataset, its covariance matrix will be the same as the correlation matrix of the pre-standardized dataset. In the following Python code, we first import the necessary packages for matrix manipulation and for eigenvector/eigenvalue calculation. Then, we will get the covariance matrix for the standardized data and draw a heatmap for the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfKLeDVJSHJD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2swiLJ5Tp8B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate covariance matrix of the standardized dataset\n",
    "std_data_cov = standardized_data.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1727915206045,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "KqtFvHTeUXt2",
    "outputId": "0eb425b8-518b-467d-fc85-3b7c85f0c406",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw a heatmap of the covariance matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(std_data_cov, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Covariance Heat Map of Standardized Treasury Bond Yields')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZprvks_QJWr"
   },
   "source": [
    "Now let's calculate the eigenvectors and eigenvalues of the covariance matrix of the standardized yield dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1727915210437,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "UBm9V8i9STck",
    "outputId": "f093eb19-9356-42b2-fcaa-a494e38d003a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate eigenvectors and eigenvalues of the covariance matrix of standardized yield dataset\n",
    "eigenvalues, eigenvectors = LA.eig(std_data_cov)\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1727915211507,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "MD6zxlbTT5v4",
    "outputId": "4ef50051-cebf-4788-fdd6-fab072e55bf0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5ZlhDL6Q2s0"
   },
   "source": [
    "From the above output, we can see that the eigenvalues and corresponding eigenvectors are ordered in descending order by the values of eigenvalues. In PCA, we also call eigenvectors **loadings**. We will see why this is important later.\n",
    "<br>\n",
    "\n",
    "We can view this collection of all eigenvectors as a linear transformation matrix to the standardized dataset. The transformed data will have a very interesting feature that we will introduce soon. Let's transform the standardized data with eigenvectors first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1727915216666,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "JUmDtSWCXnqH",
    "outputId": "4bccdaec-90f9-47cc-baea-980c18d04baa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform standardized data with Loadings\n",
    "principal_components = standardized_data.dot(eigenvectors)\n",
    "principal_components.columns = [\"PC_1\",\"PC_2\",\"PC_3\",\"PC_4\",\"PC_5\",\"PC_6\",\"PC_7\",\"PC_8\",\"PC_9\",\"PC_10\",\"PC_11\"]\n",
    "principal_components.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5nTEW67XiMm"
   },
   "source": [
    "The above result shows 11 transformed variables. In PCA, they are called **principal components**. Remember we mentioned earlier that the eigenvalues from PCA are in descending order. These principal components are also presented in the same order as their corresponding eigenvalues. For example, PC_1 corresponds to the first eigenvalue, which is 9.22. PC_2 corresponds to the second highest eigenvalue, which is 1.63.\n",
    "<br>\n",
    "\n",
    "The most important feature of PCA is **the leading principal components can explain higher portions of the variance of the dataset than the rest of the principal components**. For example, PC_1 can explain more variance in the standardized data than PC_2. But by how much? The corresponding eigenvalue for PC_1 is the variance of the whole data explained by PC_1. It is 9.22 in this case. By the same logic, PC_2 explains 1.63 of the total variance of the dataset. We can sum up all the eigenvalues to get the total variance of the data. From the total variance of the data, we can also calculate the percentage of variance contribution each principal component catches. Here is the Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1727915224081,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "TEwx0ZIAV1yE",
    "outputId": "f4e5720d-56eb-4ce4-d51b-bff04dcd0604",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put data into a DataFrame\n",
    "df_eigval = pd.DataFrame({\"Eigenvalues\":eigenvalues}, index=range(1,12))\n",
    "\n",
    "# Work out explained proportion\n",
    "df_eigval[\"Explained proportion\"] = df_eigval[\"Eigenvalues\"] / np.sum(df_eigval[\"Eigenvalues\"])\n",
    "#Format as percentage\n",
    "df_eigval.style.format({\"Explained proportion\": \"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_20Gp4cUt03"
   },
   "source": [
    "From the above table, we can see PC_1 can explain almost 84% of the variance in the standardized data. PC_2 can explain almost 15% of the variance in the standardized data. The first two leading principal components can explain almost 99% of the variance in the dataset. The rest of the 9 principal components only explain 1% of the variance of the data. Hence, to make data analysis more efficient without losing too much information, we can just use the first two principal components for analysis instead of all 11 principal components. Due to this special feature, we also call PCA a data-dimension reduction technique.\n",
    "<br>\n",
    "\n",
    "PCA's application is very wide in financial analysis and machine learning. For example, for image analysis, the original collection of images may be large, but by using PCA, we can reduce the data size while still preserving the key features of the images.\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSfZPVkBHqR-"
   },
   "source": [
    "### **2.5. Principal Component Analysis for U.S. Treasury Yield**\n",
    "The great thing about using PCA to analyze the U.S. Treasury yield curve is that it can decompose the yield curve into a number of principal components (Oprea). The first three leading principal components describe the following features of the yield curve:\n",
    "<br>\n",
    "1. PC_1: the parallel shift of the yield curve (shift).\n",
    "2. PC_2: the flattening or steeping of the yield curve (tilt).\n",
    "3. PC_3: Curvature change of the yield curve (twist).\n",
    "<br></n>\n",
    "\n",
    "First, let's draw the yield curve as the benchmark for better comparison (Bjerring)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "executionInfo": {
     "elapsed": 2103,
     "status": "ok",
     "timestamp": 1727919914546,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "g8cUiLxf1ZWz",
    "outputId": "35e76336-473b-4bca-c549-166cb2d67447",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Treasury Yield Curve\n",
    "yields.plot(figsize=(12, 8), title='Figure 2, Treasury Yields', alpha=0.7) # Plot the yields\n",
    "plt.legend(bbox_to_anchor=(1.03, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTWjxjTf5yzi"
   },
   "source": [
    "Now, let's draw principal components. Let's start from PC_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1727919935575,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "4gYZkrgXXh_k",
    "outputId": "4ca1b761-38cc-4d39-d79f-9929f5cf3044",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot PC_1\n",
    "principal_components[\"PC_1\"].plot(figsize=(12, 8), title='Figure 3, Principal Component 1', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8jlYb0fiTVg"
   },
   "source": [
    "From the above Figure 3 of PC_1, we can see the pattern looks similar to the patterns of Figure 2 (this graph starts from 2001 because we dropped NaN). This PC_1 is used to represent the yield curve parallel shift. Next, let's analyze PC_2. PC_2 is usually used to analyze how tilted the yield curve is. The following demonstration in Python code explains why PC_2 is used to analyze tilt.\n",
    "<br>\n",
    "\n",
    "First, let's calculate the difference of 2-year yield and 10-year yield as the yield curve tilt and draw a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1727915282988,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "O1WXM5yGZIuV",
    "outputId": "3a1b216e-c6e5-4822-f80c-30efb380f71a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate slope (difference) of 2-year Treasury yield and 10-year Treasury yield\n",
    "df_s = pd.DataFrame(data = standardized_data)\n",
    "df_s = df_s[[\"2 Year\",\"10 Year\"]]\n",
    "df_s[\"Tilt\"] = df_s[\"2 Year\"] - df_s[\"10 Year\"]\n",
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1727920014576,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "KoqWpY2sbTZq",
    "outputId": "50bbb330-b311-4baa-d543-c555f90c8d6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw the graph of Slope of 2-Year Treasury Yield - 10-Year Treasury Yield\n",
    "df_s[\"Tilt\"].plot(figsize=(12, 8), title='Figure 4, Tilt of 2-Year Treasury Yield - 10-Year Treasury Yield', alpha=0.7) # Plot the yields difference\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKxcPzUDk6Mw"
   },
   "source": [
    "After drawing Figure 4 for the yield difference between the 2-year Treasury yield and 10-year Treasury yield, let's draw a graph for PC_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1727920057063,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "inQGmP9Fbx4o",
    "outputId": "ea4a369c-0371-4509-e3c2-e4ef61d81cfa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw the graph for PC_2\n",
    "principal_components[\"PC_2\"].plot(figsize=(12, 8), title='Figure 5, Principal Component 2', alpha=0.7) # Plot the yields\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7YaZ14-lpYr"
   },
   "source": [
    "From Figure 4 and Figure 5, we can see that the tilt of the 2-year Treasury yield and 10-year Treasury yield graph has a pattern similar to the PC_2 graph. Let's calculate the correlation to confirm this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1727915336752,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "gOphT8jJcZgI",
    "outputId": "bb8fb2bc-cfad-45ac-e727-e89669e7f220",
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.corrcoef(principal_components[\"PC_2\"], df_s[\"Tilt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9IjKNtlm19C"
   },
   "source": [
    "The above code shows that the correlation of PC_2 and Tilt is 97%, which is very high. Hence, we can use the PC_2 as a proxy to analyze how tilted the yield curve is.\n",
    "<br>\n",
    "<br>\n",
    "Now let's draw PC_3, the change of curvature of the yield curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1727920128876,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "Ye96LzxpyVkV",
    "outputId": "8761725e-acef-43f5-83d9-73c14ef5b3a1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw the graph for PC_3\n",
    "principal_components[\"PC_3\"].plot(figsize=(12, 8), title='Figure 6, Principal Component 3', alpha=0.7) # Plot the yields\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glV1YwwVy2hc"
   },
   "source": [
    "From Figure 6, we can see that the change in curvature of the yield curve oscillates around 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2ScBl9v5WSL"
   },
   "source": [
    "## **3. Value at Risk for a Fixed-Income Portfolio**\n",
    "In this section, we are going to use the feature extraction method we learned in the last section to calculate the Value at Risk of a bond portfolio.\n",
    "<br>\n",
    "<br>\n",
    "### **3.1 Value at Risk (VaR)**\n",
    "**Value at Risk (VaR)** is a statistical metric to measure the potential maximum loss of an investment or a portfolio at a given time period under certain a confidence level. For example, when a stock portfolio has a VaR of \\$1 million during a day at 95% confidence level, it means that there is 95% probability that this stock portfolio will not lose over $1 million in a day.\n",
    "<br>\n",
    "VaR is an easy-to-understand risk metric and can be used to compare risks across different asset classes. VaR is usually used in risk management for portfolio management or regulartory reporting. It is also part of the metrics to set risk cap for traders. VaR can also be applied for capital allocation.\n",
    "<br>\n",
    "<br>\n",
    "### **3.2 VaR for a Simple Treasury Bond Portfolio**\n",
    "We will demonstrate how to calculate VaR for a simple Treasury bond portfolio in this section. This simple bond portfolio will only consist of 2-year Treasury bonds, 5-year Treasury bonds, and 10-year Treastury bonds. First, let's create a dataset with the yields of these three bonds and calculate the daily yield percentage change in these bonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1725376947120,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "xnKD40aqBnKx",
    "outputId": "9e0da048-7675-461d-a19c-fe4a703f0075",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataset with 3 Treasury bond yields and calculate the yield changes\n",
    "var_dataset = yields[[\"2 Year\",\"5 Year\",\"10 Year\"]]\n",
    "var_yield_chng_dataset = var_dataset.pct_change()\n",
    "var_yield_chng_dataset = var_yield_chng_dataset.dropna()\n",
    "var_yield_chng_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOeYpMFnPthf"
   },
   "source": [
    "\n",
    "\n",
    "Next, we will prepare the dataset. We need to standardize the dataset first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfWKiLxrDvNC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "var_yield_chng_dataset_means = var_yield_chng_dataset.mean()\n",
    "var_yield_chng_dataset_stds = var_yield_chng_dataset.std()\n",
    "var_yld_chng_stnd_data = (var_yield_chng_dataset - var_yield_chng_dataset_means) / var_yield_chng_dataset_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9ufo0hLQEFK"
   },
   "source": [
    "Now we can calculate the eigenvectors and eigenvalues of the standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iAaXGNFEgdi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate eienvectors and eigenvalues and rank by eigenvalues\n",
    "var_cov_matrix = var_yld_chng_stnd_data.cov()\n",
    "eigenvalues, eigenvectors = np.linalg.eig(var_cov_matrix)\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "pca_components = eigenvectors[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwiK1Mp1Z75H"
   },
   "source": [
    "Let's check eigenvalues and how much variance of the data is explained by an eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1725376956093,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "lxFNm08xQYMT",
    "outputId": "14299f5a-e4fc-4474-8abf-041130d6d44d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put data into a DataFrame\n",
    "df_eigval = pd.DataFrame({\"Eigenvalues\":eigenvalues}, index=range(1,4))\n",
    "\n",
    "# Work out explained proportion\n",
    "df_eigval[\"Explained proportion\"] = df_eigval[\"Eigenvalues\"] / np.sum(df_eigval[\"Eigenvalues\"])\n",
    "#Format as percentage\n",
    "df_eigval.style.format({\"Explained proportion\": \"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMolJjbQWnUr"
   },
   "source": [
    "From the above table, we can see the first two eigenvectors account for 97% of the variance in the dataset. Hence, we are going to select the first two eigenvectors for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1JJp6XSFXuQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose number of components (e.g., 2)\n",
    "n_components = 2\n",
    "selected_components = pca_components[:, :n_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SR0Ph1xW6G7"
   },
   "source": [
    "Next, let's assume that our simple bond portfolio consists of \\$2 million in 2-year Treasury bonds, \\$2 million in 5-year Treasury bonds, and \\$1 million in 10-year Treasury bonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4216qrpOFcCa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a simple portfolio\n",
    "portfolio = {\n",
    "    2: 2000000,  # $2M in 2-year bond\n",
    "    5: 2000000,  # $2M in 5-year bond\n",
    "    10: 1000000  # $1M in 10-year bond\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnpQhZdfXdgF"
   },
   "source": [
    "Next, we will calculate bond sensitivities in the portfolio. We assume the bond durations are the same as their maturity for simplicity. Then, we can calculate the portfolio value changes and VAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1725376965647,
     "user": {
      "displayName": "Vincent Wu",
      "userId": "18065837459387437522"
     },
     "user_tz": 240
    },
    "id": "JhHX-JIGFj7z",
    "outputId": "f9047e99-b659-4030-81a0-0bbb6eff6e9f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate portfolio sensitivities (assuming duration = maturity for simplicity)\n",
    "sensitivities = np.array([maturity * amount for maturity, amount in portfolio.items()])\n",
    "\n",
    "# Calculate portfolio value changes\n",
    "portfolio_changes = (var_yield_chng_dataset*sensitivities) @ selected_components\n",
    "\n",
    "# Calculate VaR\n",
    "confidence_level = 0.95  # 95% VaR\n",
    "var = -np.percentile(portfolio_changes, 100 * (1 - confidence_level))\n",
    "\n",
    "print(f\"1-day 95% VaR: ${var:,.2f}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Portfolio Value: ${sum(portfolio.values()):,.2f}\")\n",
    "print(f\"VaR as % of Portfolio Value: {var / sum(portfolio.values()) * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZqzlGghYDiS"
   },
   "source": [
    "The above result shows that the 1-day VaR at 95% confidence level for our simple Treasury bond portfolio is $458,249. It is about 9% of the total portfolio value. The above example demonstrates how to use the feature extraction method to reduce the portfolio dataset and use the smaller dataset to calculate VaR.\n",
    "<br>\n",
    "<br>\n",
    "## **4. Conclusion**\n",
    "In this lesson, we first went through the basics of bivariate analysis. We explained what bivariate analysis is and then we introduced the concepts of covariance and correlation. We then moved on to applying the feature extraction method to analyze the Treasury yield curve. We learned how to standardize a dataset. We also learned what eigenvectors and eigenvalues are. Then, we moved to conduct feature extraction from the Treasury bond yield. Next, we also applied the feature extraction method to calculate the Value at Risk of a simple Treasury bond portfolio. These tools are fundamental for understanding more advanced financial theories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DprIXlRc0nlP"
   },
   "source": [
    "**References**\n",
    "<br>\n",
    "* Bjerring, Thomas T. \"The Yield Curve and Its Components.\" *Github*, 16 October 2019, https://bjerring.github.io/bonds/2019/10/16/the-yield-curve-and-its-components.html.\n",
    "<br>\n",
    "* Oprea, Andreea. \"The Use of Principal Component Analysis (PCA) in Building Yield Curve Scenarios and Identifying Relative-Value Trading Opportunities on the Romanian Government Bond Market.\" *Journal of Risk and Financial Management*, vol. 15, no. 6, 2022. https://www.mdpi.com/1911-8074/15/6/247.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WMl6c9_0obx"
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
